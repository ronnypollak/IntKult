% supported fields: https://www.bibtex.com/s/bibliography-style-base-alpha/

%    address
%    author
%    booktitle
%    chapter
%    edition
%    editor
%    howpublished
%    institution
%    journal
%    key
%    month
%    note
%    number
%    organization
%    pages
%    publisher
%    school
%    series
%    title
%    type
%    volume
%    year

% see also: https://www.bibtex.com/styles/

% howpublished = {available at: \url{https://...} (successful access: Juli 2022)},


@book{khan_guide_2018,
	location = {Cham},
	title = {A Guide to Convolutional Neural Networks for Computer Vision},
	isbn = {978-3-031-00693-7 978-3-031-01821-3},
	url = {https://link.springer.com/10.1007/978-3-031-01821-3},
	series = {Synthesis Lectures on Computer Vision},
	publisher = {Springer International Publishing},
	author = {Khan, Salman and Rahmani, Hossein and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
	urldate = {2023-01-20},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-031-01821-3},
}

@article{ferrer_bias_2021,
	title = {Bias and Discrimination in {AI}: A Cross-Disciplinary Perspective},
	volume = {40},
	issn = {1937-416X},
	doi = {10.1109/MTS.2021.3056293},
	shorttitle = {Bias and Discrimination in {AI}},
	abstract = {Operating at a large scale and impacting large groups of people, automated systems can make consequential and sometimes contestable decisions. Automated decisions can impact a range of phenomena, from credit scores to insurance payouts to health evaluations. These forms of automation can become problematic when they place certain groups or people at a systematic disadvantage. These are cases of discrimination-which is legally defined as the unfair or unequal treatment of an individual (or group) based on certain protected characteristics (also known as protected attributes) such as income, education, gender, or ethnicity. When the unfair treatment is caused by automated decisions, usually taken by intelligent agents or other {AI}-based systems, the topic of digital discrimination arises. Digital discrimination is prevalent in a diverse range of fields, such as in risk assessment systems for policing and credit scores [1], [2].},
	pages = {72--80},
	number = {2},
	journaltitle = {{IEEE} Technology and Society Magazine},
	author = {Ferrer, Xavier and Nuenen, Tom van and Such, Jose M. and Coté, Mark and Criado, Natalia},
	date = {2021-06},
	note = {Conference Name: {IEEE} Technology and Society Magazine},
	keywords = {Artificial intelligence, Automation, Education, Insurance, Intelligent agents, Risk management, Systematics},
}

@article{bacchini_race_2019,
	title = {Race, again: how face recognition technology reinforces racial discrimination},
	volume = {17},
	issn = {1477-996X},
	url = {https://doi.org/10.1108/JICES-05-2018-0050},
	doi = {10.1108/JICES-05-2018-0050},
	shorttitle = {Race, again},
	abstract = {Purpose This study aims to explore whether face recognition technology – as it is intensely used by state and local police departments and law enforcement agencies – is racism free or, on the contrary, is affected by racial biases and/or racist prejudices, thus reinforcing overall racial discrimination.Design/methodology/approach The study investigates the causal pathways through which face recognition technology may reinforce the racial disproportion in enforcement; it also inquires whether it further discriminates black people by making them experience more racial discrimination and self-identify more decisively as black – two conditions that are shown to be harmful in various respects.Findings This study shows that face recognition technology, as it is produced, implemented and used in Western societies, reinforces existing racial disparities in stop, investigation, arrest and incarceration rates because of racist prejudices and even contributes to strengthen the unhealthy effects of racism on historically disadvantaged racial groups, like black people.Practical implications The findings hope to make law enforcement agencies and software companies aware that they must take adequate action against the racially discriminative effects of the use of face recognition technology.Social implications This study highlights that no implementation of an allegedly racism-free biometric technology is safe from the risk of racially discriminating, simply because each implementation leans against our society, which is affected by racism in many persisting ways.Originality/value While the ethical survey of biometric technologies is traditionally framed in the discourse of universal rights, this study explores an issue that has not been deeply scrutinized so far, that is, how face recognition technology differently affects distinct racial groups and how it contributes to racial discrimination.},
	pages = {321--335},
	number = {3},
	journaltitle = {Journal of Information, Communication and Ethics in Society},
	author = {Bacchini, Fabio and Lorusso, Ludovica},
	urldate = {2023-01-14},
	date = {2019-01-01},
	note = {Publisher: Emerald Publishing Limited},
}

@report{usa,
	author = {Muñoz, Cecilia and Smith, Megan and Patil, DJ},
	title = {Big Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights},
type = {•},
	institution = {Executive Office of the President},
	year = {2016},
	month = {may}

}


@inproceedings{buolamwini_gender_2018,
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	shorttitle = {Gender Shades},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, {IJB}-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for {IJB}-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	eventtitle = {Conference on Fairness, Accountability and Transparency},
	pages = {77--91},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	publisher = {{PMLR}},
	author = {Buolamwini, Joy and Gebru, Timnit},
	urldate = {2023-01-20},
	date = {2018-01-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{nanni2022feature,
  title={Feature transforms for image data augmentation},
  author={Nanni, Loris and Paci, Michelangelo and Brahnam, Sheryl and Lumini, Alessandra},
  journal={arXiv preprint arXiv:2201.09700},
  year={2022}
}

@online{singh_face_2020,
	title = {Face Data Augmentation Techniques},
	url = {https://manmeet3.medium.com/face-data-augmentation-techniques-ace9e8ddb030},
	abstract = {This article is a brief outline on various state-of-art techniques used for face data augmentation.},
	titleaddon = {Medium},
	author = {Singh, Manmeet},
	urldate = {2023-01-20},
	date = {2020-05-25},
	langid = {english},
}

@incollection{yang_cross-datasets_2022,
	location = {Singapore},
	title = {Cross-Datasets Evaluation of Machine Learning Models for Intrusion Detection Systems},
	volume = {217},
	isbn = {9789811621017 9789811621024},
	url = {https://link.springer.com/10.1007/978-981-16-2102-4_73},
	abstract = {The conventional way to evaluate the performance of machine learning models intrusion detection systems ({IDS}) is by using the same dataset to train and test. This method might lead to the bias from the computer network where the traﬃc is generated. Because of that, the applicability of the learned models might not be adequately evaluated. We argued in [1] that a better way is to use cross-datasets evaluation, where we use two diﬀerent datasets for training and testing. Both datasets should be generated from various networks. Using this method as it was shown in [1] may lead to a signiﬁcant drop in the performance of the learned model. This indicates that the models learn very little knowledge about the intrusion, which would be transferable from one setting to another. The reasons for such behaviour were not fully understood in [1]. In this paper, we investigate the problem and show that the main reason is the diﬀerent deﬁnitions of the same feature in both models. We propose the correction and further empirically investigate cross-datasets evaluation for various machine learning methods. Further, we explored cross-dataset evaluation in the multi-class classiﬁcation of attacks, and we show for the most models that learning traﬃc normality is more robust than learning intrusions.},
	pages = {815--828},
	booktitle = {Proceedings of Sixth International Congress on Information and Communication Technology},
	publisher = {Springer Singapore},
	author = {Al-Riyami, Said and Lisitsa, Alexei and Coenen, Frans},
	editor = {Yang, Xin-She and Sherratt, Simon and Dey, Nilanjan and Joshi, Amit},
	urldate = {2023-01-21},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-981-16-2102-4_73},
	note = {Series Title: Lecture Notes in Networks and Systems},
}

@inproceedings{chen_cdevalsumm_2020,
	location = {Online},
	title = {{CDEvalSumm}: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems},
	url = {https://aclanthology.org/2020.findings-emnlp.329},
	doi = {10.18653/v1/2020.findings-emnlp.329},
	shorttitle = {{CDEvalSumm}},
	abstract = {Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways (i.e. abstractive and extractive) on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in https://github.com/zide05/{CDEvalSumm}.},
	eventtitle = {Findings 2020},
	pages = {3679--3691},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Yiran and Liu, Pengfei and Zhong, Ming and Dou, Zi-Yi and Wang, Danqing and Qiu, Xipeng and Huang, Xuanjing},
	urldate = {2023-01-21},
	date = {2020-11},
}

@article{fairness,
author = {Pessach, Dana and Shmueli, Erez},
title = {A Review on Fairness in Machine Learning},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3494672}, <span><a href="javascript:"><img class="citavipicker" identifier="10.1145/3494672}," identifiertype="1" src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz48IURPQ1RZUEUgc3ZnIFBVQkxJQyAiLS8vVzNDLy9EVEQgU1ZHIDEuMS8vRU4iICJodHRwOi8vd3d3LnczLm9yZy9HcmFwaGljcy9TVkcvMS4xL0RURC9zdmcxMS5kdGQiPjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iRWJlbmVfMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeD0iMHB4IiB5PSIwcHgiIHdpZHRoPSIxNnB4IiBoZWlnaHQ9IjE2cHgiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZW5hYmxlLWJhY2tncm91bmQ9Im5ldyAwIDAgMTYgMTYiIHhtbDpzcGFjZT0icHJlc2VydmUiPjxnPjxnPjxwYXRoIGZpbGw9IiNGRkZGRkYiIGQ9Ik04LjAwMSwxNS41QzMuODY0LDE1LjUsMC41LDEyLjEzNiwwLjUsOGMwLTQuMTM1LDMuMzY1LTcuNSw3LjUwMS03LjVTMTUuNSwzLjg2NCwxNS41LDhTMTIuMTM3LDE1LjUsOC4wMDEsMTUuNXoiLz48cGF0aCBmaWxsPSIjRDUyQjFFIiBkPSJNOC4wMDEsMUMxMS44NiwxLDE1LDQuMTQxLDE1LDhzLTMuMTM5LDctNi45OTksN0M0LjE0LDE1LDEsMTEuODU5LDEsOFM0LjE0LDEsOC4wMDEsMSBNOC4wMDEsMEMzLjU4MiwwLDAsMy41ODIsMCw4czMuNTgyLDgsOC4wMDEsOEMxMi40MTgsMTYsMTYsMTIuNDE4LDE2LDhTMTIuNDE4LDAsOC4wMDEsMEw4LjAwMSwweiIvPjwvZz48cGF0aCBmaWxsPSIjRDUyQjFFIiBkPSJNNi43NDUsMTIuNTg5Yy0wLjIyNywwLjEyMi0wLjQ5NywwLjI0Ny0wLjY4NCwwLjI0N2MtMC4zMTgsMC0wLjUwMS0wLjE2NC0wLjUwMS0wLjQ1MmMwLTAuMjA3LDAuMTQtMC4zNzUsMC41OTUtMC42MjJjMS41NDktMC45MDQsMi41OTQtMi4yNzIsMi41OTQtMy43MjFjMC0wLjgyNS0wLjIyNy0xLjExOS0wLjY4MS0xLjExOWMtMC4xMzUsMC0wLjMyLDAuMjE5LTAuNjM2LDAuMjE5SDcuMTU3QzYuMTAyLDcuMTQzLDUuMzMzLDYuMjY0LDUuMzMzLDUuMjNjMC0xLjE1MiwwLjk1OC0yLjAwNiwyLjI4LTIuMDA2YzEuNzc3LDAsMy4wNTMsMS4zNzMsMy4wNTMsMy40M0MxMC42NjYsOS4yMTUsOS4yMDMsMTEuMjcsNi43NDUsMTIuNTg5Ii8+PC9nPjwvc3ZnPg" style="border: 0px none!important;width: 16px!important;height: 16px!important;margin-left:1px !important;margin-right:1px !important;" title="Titel anhand dieser DOI in Citavi-Projekt übernehmen" existsinproject="0"></a></span>
doi = {10.1145/3494672},
abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {51},
numpages = {44},
keywords = {algorithmic fairness, fairness-aware machine learning, Algorithmic bias, fairness in machine learning}
}


